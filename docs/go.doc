26-09-25


Today my goal was to implement connection pooling adn then ssl request and cancelling request and i thought it would be easy af as it is already a well defined pgxpool library but it wasn't easy at all lets get into it 


pgxpool provides us with 
It lives inside your Go process.

It maintains a pool of physical connections to Postgres.

When you pool.Acquire(ctx), you get a dedicated backend connection from the pool.

You hold that connection until you Release() it.

During that time, no other goroutine can use that same physical connection.


The problem here is that it does not do transactional pooling - where the pooling fconnection is based on the transaction and held only for one transaction the proxy only gives you a physical connection for the duration of a transaction.

After COMMIT / ROLLBACK, the backend connection is returned to the pool.

The next client transaction might reuse the same backend.


 or statement pooling is pooling for each statemnt 


 So the pooling that pgxpool provides is basically a client side pooling 


And since we are using go routines here i need to create a global connection pooling stratefy so that each go routnine does not end up creating it's own pool, i need to maintain the pool for the whole rds instacnce 

To do that firstly we need to pass some config to the pool like max oxnnections it can open etc and we have passed this config right now 

const defaultMaxConns = int32(7)
	const defaultMinConns = int32(0)
	const defaultMaxConnLifetime = time.Hour
	const defaultMaxConnIdleTime = time.Minute * 30
	const defaultHealthCheckPeriod = time.Minute
	const defaultConnectTimeout = time.Second * 5


Then since we can have multiple go routines talking to a single connection pool and our logic has to be such that the first connection pool is created byy the first go routine that is started and after that all the go routines need to use the same connection pool and can not create it's own
To handle this we will use mutexes as we cannot create a race condition between the go routines if multiple are created at the same time as it would cause a race condition for the first one to create poool 

You want to keep a pool of connections per database name (poolManager[database]).

Multiple goroutines may request a pool at the same time.

You don‚Äôt want to create duplicate pools.

You also don‚Äôt want to lock too aggressively (hurts performance).

The pool manabge is basically a map of string of the pool connections that will get created for each db poolManager = make(map[string]*pgxpool.Pool)

poolManager = map[string]*pgxpool.Pool{
    "postgres":   pool1,  // Pool for 'postgres' database
    "myapp_db":   pool2,  // Pool for 'myapp_db' database  
    "analytics":  pool3,  // Pool for 'analytics' database
}



Step 1: Try fast read with RLock
poolMutex.RLock()
pool, exists := poolManager[database]
poolMutex.RUnlock()

if exists {
    return pool, nil
}


RLock allows multiple goroutines to read simultaneously.

If the pool already exists, we return immediately.

This makes the function very fast when pools are already created.


Step 2: Acquire write lock
poolMutex.Lock()
defer poolMutex.Unlock()


If the pool didn‚Äôt exist, we need to create one.

For this, only one goroutine at a time can be in this section.

defer ensures we always unlock when the function exits.

Step 3: Double-check (avoid duplicate work)
if pool, exists := poolManager[database]; exists {
    return pool, nil
}


Why check again? Because another goroutine might have created the pool while we were waiting for the lock.

Without this, we‚Äôd create multiple pools for the same database.


Step 4: Safe creation
pool = pgxpool.New(...)
poolManager[database] = pool
return pool, nil


Now we‚Äôre the only goroutine holding the write lock.

Safe to create a new pool and store it in the map.

After this, all future calls will find it at Step 1.


pgxpool maintains a pool of connections in memory.

By default:

min_conns = 0

max_conns = 4 (unless you set pool_max_conns in the connection string)

When you pool.Acquire(ctx), it either:

Hands you an existing connection from the pool (if available), or

Creates a new connection (if under max_conns), or

Waits until a connection is free (if at capacity).

So, verifying it means proving:

Multiple goroutines share fewer DB connections than goroutines.

Connections are reused instead of re-created each time.

Once this is done, the pool is created succesffuly when the backend needs to connet to the postgres server it needs to opena  connection to the psql db and this is done via the AcquireConnection() where it acquires a connection via the Acquire() functio from pgxpool and returns that connection to the connect function


Once the proxy needs to connect to the backend there is a bit of a complicated process that happens here because of a cetain race condition 


The proxy needs some service account user creds for the proxy to connect to the backend and make a connetion pool 
Pool creates connection ‚Üí Authenticates as GPRXY_USER

But the issue that occured here is that when the When you use pgxpool, the connection is already authenticated and established at the pool level using your service user credentials. But then you're trying to run the authentication flow again with the client's credentials over an already-authenticated connection.


So the best practive in this case scneaior is to Pool connections authenticate as a service user
the proxy  forwards the authentication to postgresql and handles it properly and if its a success and since it's the first go routine it creates a pool 

Client connects ‚Üí "I'm alice with password xyz123"
Proxy creates temp connection ‚Üí Forwards to PostgreSQL
PostgreSQL validates ‚Üí "Password correct" or "Password wrong"
If auth succeeds ‚Üí Proxy gets pooled connection for queries
If auth fails ‚Üí Connection rejected


So the code would basically be like 

// 1. Create a TEMPORARY connection for authentication
        tempConn, err := pc.createTempConnection(database, user)
        if err != nil {
            return pc.sendErrorToClient(pgconn, "Database unavailable")
        }
        defer tempConn.Close()
        
        // 2. Run FULL authentication flow with PostgreSQL
        err = pc.AuthenticateUser(pgconn, tempConn, msg)
        if err != nil {
            return err // Authentication failed
        }
        
        // 3. ONLY after successful auth, get pooled connection
        err = pc.connectBackend(database, user)
        if err != nil {
            return pc.sendErrorToClient(pgconn, "Database unavailable")
        }
        
        // 4. Set up for query forwarding
        underlyingConn := pc.poolConn.Conn().PgConn().Conn()
        pc.bf = pgproto3.NewFrontend(pgproto3.NewChunkReader(underlyingConn), underlyingConn)



Later for me to implement custom authentication as well it would be similar where the first authenication i would have to handle via 

// AWS RDS Proxy approach - custom authentication
func (pc *gprxyConn) authenticateWithIAM(user, token string) error {
    // Validate IAM token
    return aws.ValidateIAMToken(user, token)
}


So the service user i have setup currently is the postgres user directly which is the super user but ideally it should be a user with a least priveleges priciple folloowing user 


Now after writing all this code i wanted to start testing the proxy on whether everythingis working as expected and this is where i face my next big challenge for which i ahve to write code tomorrow

So the issue i was facing is that i was getting this error log saying 

received from PostgreSQL: *pgproto3.AuthenticationSASL
unknown message type: *pgproto3.AuthenticationSASL
waiting for client response
forwarding client response: *pgproto3.PasswordMessage
received from PostgreSQL: *pgproto3.ErrorResponse
PostgreSQL auth error: insufficient data left in message

And with this the password auth failed for the postgres user multiple times whereras im able to connect normally without proxy 

and digging deeper i understood and found out that there is an encryption method that is used by the postgres db to encrup the passwords and this by default is  scram-sha-256


PostgreSQL 17 uses SCRAM-SHA-256 (SASL) by default, not MD5
Your proxy didn't recognize AuthenticationSASL as a valid message type
Client was sending wrong response format for SASL authentication

So i thouhg ti need to add SASL support and authenitcaiton as well and the backend will handle it as expected 

and to just verify all this was correct i created a dummy user to check if it would work as expected 

SET password_encryption = 'md5';
CREATE USER testuser WITH PASSWORD 'testpass';
GRANT ALL PRIVILEGES ON DATABASE postgres TO testuser;

and lo behlod when i connect with this psql "postgresql://testuser@localhost:7777/postgres" the password auth worked fine and i was in the db without any errors 

so now we know that this was the actaul issue 

The difference is in when and how the passwords were created:
postgres user: Created with SCRAM-SHA-256 password hash (before we changed settings)
testuser: Created with MD5 password hash (after we set password_encryption = 'md5')
Even though SHOW password_encryption shows scram-sha-256 for both users, that's the current setting, not how their existing passwords are stored.


So the obvious options of solutions here was to defaulkt every user to use MD5 or add SSAL support to my proxy and obviously i cannot change the default psql db config for eferthing that woudl defeat the prupise so have to add SASL supoport now 


but our main issue in the code was that The problem is not with your proxy - it's with the client-proxy interaction. Here's what's happening:
PostgreSQL ‚Üí Proxy: "Use SCRAM-SHA-256 authentication"
Proxy ‚Üí Client: Forwards the SASL request
Client ‚Üí Proxy: Sends simple PasswordMessage (wrong!)
Proxy ‚Üí PostgreSQL: Forwards the wrong message type
PostgreSQL: "This isn't a SASL message!" ‚Üí Error


so we had to intercept the clien'ts password message and convert it to proper SASL messages -- i think beacuse im fully confused right now will continue this tomorrow


27-09-25

Okay so what is currently happening is that 

1. Client "postgres" authenticates ‚Üí ‚úÖ Authentication succeeds
2. Proxy gets pooled connection as "testuser" ‚Üí ‚úÖ Connection established  
3. Client runs: SELECT * FROM users ‚Üí Executed as "testuser" ‚ùå
4. Database sees: "testuser ran this query" (not "postgres")

The Problem:
Authentication: Client proves they are "postgres"
Authorization: All queries run as "testuser"
Audit logs: Show "testuser" performed all actions
Row-level security: Applied based on "testuser", not "postgres"



## **2. The Complete Flow - What's Actually Happening**

### **Phase 1: Client Authentication (Temporary Connection)**
```
1. Client connects as "testuser" ‚Üí Proxy
2. Proxy ‚Üí PostgreSQL (temp connection): "I'm testuser with password"
3. PostgreSQL ‚Üí Proxy: "testuser authenticated successfully"
4. Proxy ‚Üí Client: "Authentication OK"
5. Temp connection closed
```

### **Phase 2: Service User Connection (Pool Connection)**
```
1. Proxy calls connectBackend()
2. Uses config.BuildConnectionString() ‚Üí "postgres://testuser:testpass@localhost:5432/cloudfront_data"
3. Pool creates connection as "testuser" (service user)
4. All subsequent queries use this service user connection
```

## **3. The Issues You're Facing**

### **Issue 1: User Identity Confusion**
```go
// In your logs, you see:
parameter status - session_authorization: testuser
```
**Problem**: Both client and service user are "testuser", so you can't see the separation!

### **Issue 2: No Real User Identity Preservation**
```go
// Line 141: Client user stored for logging only
pc.user = user // Store user for logging

// Lines 188, 191, etc: Queries logged with client user
log.Printf("[%s] [%s] QUERY: %s", clientAddr, pc.user, query.String)
```
**Problem**: Queries are **logged** as client user but **executed** as service user!

### **Issue 3: Authentication vs Operation Identity Mismatch**
When different users connect:
```
Client "postgres" ‚Üí Authenticates as "postgres" ‚úÖ
But queries execute as "testuser" (service user) ‚ùå
```

## **4. Detailed Service User Usage Trace**

### **Step-by-Step Service User Flow:**

1. **Startup**: `config.Load()` reads `GPRXY_USER=testuser` from .env
2. **Client connects**: Authentication happens with client credentials (temporary)
3. **Backend connection**: `connectBackend()` called
4. **Connection string built**: `postgres://testuser:testpass@localhost:5432/cloudfront_data`
5. **Pool connection**: All connections created as "testuser"
6. **Query execution**: All SQL runs as "testuser"

### **Evidence from Your Logs:**
```
Created a new connection pool for database: cloudfront_data  ‚Üê Service user pool
[localhost] acquired connection from pool for database: cloudfront_data  ‚Üê Service user connection
Pool stats - Total: 1, Acquired: 1, Idle: 0  ‚Üê Service user pool stats
```

No ‚Äî pgxpool does not preserve the identity of the client user.
All queries run under the database user that you used in the pool‚Äôs connection string.

So:

The Go client ‚Üí connects to pgxpool with one database user/password (e.g. postgres or app_user).

Every query executed through pgxpool runs as that same user.

The application user identity (who triggered the query) is not automatically visible to PostgreSQL.

üõ† Why?

pgxpool is client-side pooling (inside your Go app).

It keeps a pool of backend connections already authenticated as the configured DB user.

When your app needs a connection, it borrows one from the pool ‚Äî no new DB login/auth happens.

This makes pooling fast, but it means Postgres only ever sees one user identity (the one in the DSN).

Comparison with Other Approaches

pgbouncer in transaction pooling:
Same ‚Äî you lose per-app user identity. Everything runs as the configured user.

pgbouncer in session pooling:
Preserves session user identity, but still usually one DB user for performance.

RDS Proxy with IAM auth:
Can map IAM identities to DB users, so the DB might see different users per connection.


What happens today (pgxpool default)

You initialized your pool with one DSN (e.g. postgres://proxy_user:secret@localhost:5432/postgres).

Every connection in that pool is logged in as proxy_user.

No matter who your client is, when a query goes to Postgres it‚Äôs run as proxy_user.

That‚Äôs why user identities aren‚Äôt preserved automatically.


If you run pgxpool in the default mode, the proxy user is the only identity that PostgreSQL will see. That means:

This user must have all the privileges required by the client queries you expect to relay.

Clients themselves are authenticated at the proxy level, not at Postgres.

The proxy user effectively acts as a service account or super-app user.

So, for example:

If your proxy serves users that only read from tables ‚Üí the proxy user must have SELECT on those tables.

If your proxy needs to insert/update/delete ‚Üí it needs INSERT/UPDATE/DELETE.

If you want it to ‚Äúact on behalf‚Äù of other users ‚Üí it needs SET ROLE or SET SESSION AUTHORIZATION rights.

If the proxy should bypass all restrictions ‚Üí make it a superuser (‚ö†Ô∏è not recommended for production).

üëâ In short: the proxy user needs the union of all permissions required by the workloads it will serve.


But this is not safe at all for my purpose as the i have seperated client based users with client based dbs and i dont think it would float with the complicance prt of it as wel 


When a client connects through your proxy, their actual Postgres username/password is not used anymore.

Instead, your proxy process opens connections to Postgres using one fixed role (the proxy user).

This means:

All queries on the database are executed as the proxy user.

Any per-user database permissions/separation you had before is lost.

If your proxy user has access to multiple databases/schemas, then by definition, the proxy process can query them ‚Äî even if the original client shouldn‚Äôt.

So yes, you‚Äôre right: unless you implement extra checks in your proxy code, your proxy user can ‚Äúsee too much.‚Äù

How production systems handle this

Application-level separation

You enforce which database/schema a given client can touch inside your proxy.

Example: client connects with their app credentials ‚Üí your proxy maps them to a certain db/schema ‚Üí proxy user only runs queries in that scope.

Downside: Postgres itself no longer enforces per-user security, you‚Äôre responsible.

Multiple pools (per database/role)

Instead of a single proxy user with global rights, you spin up a pgxpool per client role.

Each pool connects to Postgres with the real DB user credentials.

Then you keep the permission separation intact, but at the cost of more pools and more connections.

Use a connection pooler that supports transaction-pooling + auth (like PgBouncer in auth_user mode)

PgBouncer can preserve client identity for authentication and enforce role separation, even while pooling.


These are my above options to fix it and i thinnk the best option for me is to create db pools based on user connection as in my k8s setup as well 12 pods use the same user and ;ets say each make one connectoin fi the db is pooled for user wise also it provides good enough pooling advantahes 


Or maybe i can go do both actaull with like a flag or something that triggers behavior like that 

Why creating a pool per user is better

Preserves database-level permissions

Each pool connects using the actual client‚Äôs DB user credentials.

Postgres enforces that user‚Äôs access automatically ‚Äî no need for your proxy to implement extra checks.

If user1 only has access to db1, they cannot touch db2 even though the proxy exists.

Transparency

Postgres sees the real client identity in pg_stat_activity, logs, and RLS policies.

Easier to audit.

Safety

Even if there‚Äôs a bug in your proxy, the proxy can‚Äôt escalate privileges because it‚Äôs not using a superuser or global pool account.


Trade-offs

Resource usage

Each pool maintains connections. If you have many users, you can quickly exhaust Postgres‚Äôs max_connections.

Example: 100 users √ó 10 connections per pool = 1,000 backend connections.

Complexity

You need to manage a map of pools keyed by username.

Handle creation, reuse, and cleanup of pools dynamically.

Idle connections

You may have idle connections for users who aren‚Äôt active.

Could be mitigated with proper pool config (MaxConns, MinConns, idle timeout).



Another issue that is currently happening s the 

Perfect! You've identified the exact issue. Let me explain what's happening and provide detailed solutions.

# üö® **The Issue: SCRAM-SHA-256 vs MD5 Authentication Mismatch**

## **What's Happening in Your Logs:**

```
connection request - user: testuser3, database: cloudfront_data, app: psql
connecting to PostgreSQL at localhost:5432 for authentication
starting authentication relay
authentication failed - severity: FATAL, code: 08P01, message: insufficient data left in message
```

## **Root Cause Analysis:**

### **1. Password Storage Format**
When you created `testuser3` **without** setting `password_encryption = 'md5'`:

```sql
-- This creates SCRAM-SHA-256 password hash
CREATE USER testuser3 WITH PASSWORD 'testpass3';
```

**Result**: Password stored as `SCRAM-SHA-256$4096:...` (not `md5...`)

### **2. Authentication Method Selection**
PostgreSQL determines authentication method based on **stored password format**:

```
testuser3 password ‚Üí SCRAM-SHA-256 format ‚Üí PostgreSQL sends AuthenticationSASL
```

### **3. Protocol Mismatch**
Your proxy's authentication flow:

```
1. PostgreSQL ‚Üí Proxy: "Use SCRAM-SHA-256 authentication" (AuthenticationSASL)
2. Proxy ‚Üí Client: Forwards SASL request
3. Client ‚Üí Proxy: Sends PasswordMessage (simple password)
4. Proxy ‚Üí PostgreSQL: Forwards PasswordMessage
5. PostgreSQL: "This isn't a SASL message!" ‚Üí Error: "insufficient data left in message"
```

# üîç **Detailed Technical Explanation**

## **The Protocol Flow Breakdown:**

### **Expected SASL Flow:**
```
PostgreSQL ‚Üí AuthenticationSASL(mechanisms: [SCRAM-SHA-256])
Client ‚Üí SASLInitialResponse(mechanism: SCRAM-SHA-256, data: client_first_message)
PostgreSQL ‚Üí AuthenticationSASLContinue(server_first_message)
Client ‚Üí SASLResponse(client_final_message)
PostgreSQL ‚Üí AuthenticationSASLFinal(server_final_message)
PostgreSQL ‚Üí AuthenticationOk
```

### **What Actually Happens:**
```
PostgreSQL ‚Üí AuthenticationSASL(mechanisms: [SCRAM-SHA-256])
Client ‚Üí PasswordMessage(password: "testpass3")  ‚Üê WRONG MESSAGE TYPE!
PostgreSQL ‚Üí ErrorResponse("insufficient data left in message")
```

## **Why "insufficient data left in message"?**

PostgreSQL expects a **SASL message structure** but receives a **simple password message**. The SASL parser tries to read SASL-specific fields that don't exist in a PasswordMessage, causing the parsing error.

The solutions here are basically to either just use md5 encryption which would be changin db defaults which i dont wnat to do so obv going ahead with implementing SASL message 

Great question! Let me analyze your current code and explain exactly what's happening and why it's failing. Let me first examine your current implementation more closely.

[1 tool called]

Perfect! Now I can see the **exact problem** in your current code. Let me explain what's happening and why it's failing:

# üîç **Current Code Analysis**

## **What Your Code Currently Does:**

Your `relayAuthFlow` function follows this pattern:

```
1. Read message from PostgreSQL ‚Üí msg
2. IMMEDIATELY forward msg to client ‚Üí cb.Send(msg)  
3. Check if client response needed ‚Üí needsClientResponse(msg)
4. If yes, get client response and forward to PostgreSQL
```

## **The Problem: Message Order Issue**

Here's the **exact sequence** that's causing the failure:

### **Step-by-Step Breakdown:**

```
1. PostgreSQL ‚Üí Proxy: AuthenticationSASL{mechanisms: ["SCRAM-SHA-256"]}
2. Proxy ‚Üí Client: AuthenticationSASL (forwarded immediately at line 59)
3. Client ‚Üí Proxy: PasswordMessage("testpass3") ‚Üê CLIENT SENDS WRONG MESSAGE TYPE!
4. Proxy ‚Üí PostgreSQL: PasswordMessage("testpass3") (forwarded at line 95)
5. PostgreSQL: "Expected SASLInitialResponse, got PasswordMessage" ‚Üí ERROR!
```

## **Why This Happens:**

### **The `psql` Client Behavior:**
- When `psql` receives `AuthenticationSASL`, it **should** send `SASLInitialResponse`
- But `psql` sometimes sends `PasswordMessage` instead (especially with simple password prompts)
- Your proxy **blindly forwards** whatever the client sends

### **PostgreSQL's Expectation:**
- PostgreSQL sent `AuthenticationSASL` ‚Üí expects `SASLInitialResponse` 
- Gets `PasswordMessage` instead ‚Üí tries to parse it as SASL ‚Üí **"insufficient data left in message"**

# üö® **The Core Issue: No Protocol Translation**

Your current code is a **transparent relay** - it forwards messages as-is without understanding or translating them.

## **What You Have:**
```go
// Lines 89-99: Blind forwarding
if needsClientResponse(msg) {
    clientMsg, err := cb.Receive()  // Could be PasswordMessage
    // ...
    err = bf.Send(clientMsg)        // Forwards PasswordMessage to PostgreSQL
}
```

## **What You Need:**
```go
// Protocol-aware handling
switch pgMsg := msg.(type) {
case *pgproto3.AuthenticationSASL:
    // Handle SASL specifically
    clientMsg := getClientResponse()
    
    switch clientResponse := clientMsg.(type) {
    case *pgproto3.PasswordMessage:
        // Convert to SASLInitialResponse
        saslMsg := convertPasswordToSASL(clientResponse.Password)
        bf.Send(saslMsg)  // Send converted message
    case *pgproto3.SASLInitialResponse:
        // Forward as-is
        bf.Send(clientMsg)
    }
}
```

# üìä **Why You Need to Split Authentication Types**

## **Current Problem:**
Your `needsClientResponse()` function treats all auth types the same:

```go
case *pgproto3.AuthenticationMD5Password,      // Expects: PasswordMessage
     *pgproto3.AuthenticationCleartextPassword, // Expects: PasswordMessage  
     *pgproto3.AuthenticationSASL,              // Expects: SASLInitialResponse ‚Üê DIFFERENT!
     *pgproto3.AuthenticationSASLContinue,      // Expects: SASLResponse ‚Üê DIFFERENT!
     *pgproto3.AuthenticationSASLFinal:         // Expects: No response ‚Üê DIFFERENT!
    return true  // "All need client response" ‚Üê WRONG!
```

## **The Reality:**
Different auth types expect **different message types**:

| PostgreSQL Sends | Client Should Send | Your Code Forwards |
|------------------|-------------------|-------------------|
| `AuthenticationMD5Password` | `PasswordMessage` | ‚úÖ `PasswordMessage` |
| `AuthenticationSASL` | `SASLInitialResponse` | ‚ùå `PasswordMessage` |
| `AuthenticationSASLContinue` | `SASLResponse` | ‚ùå `PasswordMessage` |
| `AuthenticationSASLFinal` | *Nothing* | ‚ùå Waits for response |

# üîß **The Message Discrepancy Source**

## **In Your Logs:**
```
authentication failed - severity: FATAL, code: 08P01, message: insufficient data left in message
```

## **What PostgreSQL Is Doing:**
```c
// PostgreSQL's SASL message parser (simplified)
if (msgtype == 'p') {  // SASLInitialResponse expected
    mechanism = read_string(msg);     // Tries to read mechanism name
    data_len = read_int32(msg);       // Tries to read data length
    data = read_bytes(msg, data_len); // Tries to read SASL data
    // ERROR: PasswordMessage doesn't have this structure!
}
```

## **PasswordMessage Structure:**
```
PasswordMessage: ['p'] [length] [password_string] [null_terminator]
```

## **SASLInitialResponse Structure:**
```
SASLInitialResponse: ['p'] [length] [mechanism_string] [null] [data_length] [sasl_data]
```

**PostgreSQL tries to parse PasswordMessage as SASLInitialResponse ‚Üí structure mismatch ‚Üí parsing error!**

# üí° **Why Your Current SASL Support Doesn't Work**

You have `AuthenticationSASL` in `needsClientResponse()`, but:

1. **No message type checking** - you don't verify what the client actually sends
2. **No protocol translation** - you forward PasswordMessage when PostgreSQL expects SASLInitialResponse  
3. **No SASL conversation handling** - SASL has multiple round trips (Initial ‚Üí Continue ‚Üí Final)

# üéØ **The Solution**

You need **protocol-aware authentication handling** instead of blind message forwarding:

```go
switch authType := msg.(type) {
case *pgproto3.AuthenticationSASL:
    return handleSASLAuth(cb, bf, authType)  // SASL-specific logic
case *pgproto3.AuthenticationMD5Password:
    return handleMD5Auth(cb, bf, authType)   // MD5-specific logic
}
```

This is why the split implementation is necessary - each authentication method has **different message expectations and conversation flows** that require specific handling logic.

Your current code assumes all auth methods work the same way (send request ‚Üí get password ‚Üí forward), but SASL is fundamentally different (multi-step conversation with specific message types).


